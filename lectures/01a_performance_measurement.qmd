---
title: "__  Performance and Measurement"
format:
  html: default
  revealjs: 
    chalkboard: true
    output-file: "revealjs-performance.html"
    scrollable: true

---

# Measurement 


##example 1

lets look at the paper 

[ Producing Wrong Data Without Doing Anything Obviously Wrong!](<https://dl.acm.org/citation.cfm?id=1508275>) Todd Mytkowicz, Amer Diwan, Matthias Hauswirth, and Peter F. Sweeney. ASPLOS 2009.

445 references 


---

1. Measurement bias is significant
1. Changing aspects of an experimental setup can introduce measurement bias. ​
Measurement bias is unpredictable and there are no obvious ways to avoid it. ​
Prior work in computer system evaluation does not adequately consider measurement bias. ​
1. The paper discusses two techniques for dealing with measurement bias: experimental setup randomization and causal analysis. ​
1. Measurement bias occurs for all benchmarks and architectures. ​
1. Measurement bias due to link order can significantly fluctuate conclusions. ​
1. Measurement bias due to UNIX environment size can lead to conflicting conclusions. ​
1. o avoid measurement bias, it is important to use diverse evaluation workloads, randomize the experimental setup, conduct causal analysis, and collect more information from hardware manufacturers. ​
 ---

A sample blog post about this paper
[blog](https://www.cs.cornell.edu/courses/cs6120/2019fa/blog/measurement/)

## another example

[Strangely, Matrix Multiplications on GPUs Run Faster When Given "Predictable" Data!](https://www.thonking.ai/p/strangely-matrix-multiplications)

[SIGPLAN Empirical Evaluation Guidelines](https://www.sigplan.org/Resources/EmpiricalEvaluation/)